{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["VbtPoYEd8aVa","qQUs9Jh_DO0D","y3GV7r-A8xhe","g76RYDxs9Db4","uWBI5gGg-M0n","6xBOJebe-kY-","XM34rU-A-trd","bn00FGvu_BU3"],"authorship_tag":"ABX9TyOOZfOzVBL4hyIVtVFGVqv9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Importing Libraries"],"metadata":{"id":"VbtPoYEd8aVa"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor"],"metadata":{"id":"GIjdjF8W8lCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Settings"],"metadata":{"id":"qQUs9Jh_DO0D"}},{"cell_type":"code","source":["# funuction to precise the university\n","def dynamic_url_to_scrape(university_id = \"Lebanese_University\"):\n","    return f\"https://www.researchgate.net/institution/{university_id}/members\""],"metadata":{"id":"0mJwVcCPDU1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proxy Setup"],"metadata":{"id":"y3GV7r-A8xhe"}},{"cell_type":"code","source":["# Proxy API Key\n","PROXY_API_KEY = 'c7b595f3-3772-4ef7-bf62-6f3953308c72'\n","\n","# Function to make requests with proxy rotation\n","def get_page_with_proxy(url):\n","    try:\n","        response = requests.get(\n","            url='https://proxy.scrapeops.io/v1/',\n","            params={\n","                'api_key': PROXY_API_KEY,\n","                'url': url,\n","            },\n","        )\n","        response.raise_for_status()\n","        return response\n","    except requests.exceptions.RequestException as e:\n","        raise Exception(f\"Request failed: {e}\")"],"metadata":{"id":"L4DzWhxy85Jv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scrapping functions"],"metadata":{"id":"g76RYDxs9Db4"}},{"cell_type":"code","source":["# Function to extract maximum page number from pagination\n","def get_maximuim_page_number(soup):\n","    pagination_elements = soup.find_all(class_=\"nova-legacy-c-pagination__item\")\n","    if pagination_elements:\n","        return int(pagination_elements[-1].get_text(strip=True))\n","    return 1  # Default to 1 if no pagination is found\n","\n","# Function to extract doctors divs\n","def extract_doctors_divs(soup):\n","    return soup.find_all(class_=\"nova-legacy-o-stack__item institution-members-list\")\n","\n","# Function to extract disciplines\n","def extract_disciplines(div):\n","    dis_str = \", \".join(discipline.get_text(strip=True) for discipline in div)\n","    return dis_str\n","\n","# Function to extract basic doctor information\n","def extract_doctor_basic_info(doctor_div):\n","    try:\n","        doctor_link = doctor_div.find('a', href=True)\n","        doctor_id = doctor_link['href'].split('/')[-1] if doctor_link else None\n","        doctor_name = doctor_div.find('a', class_=\"nova-legacy-e-link\").get_text(strip=True) if doctor_div.find('a', class_=\"nova-legacy-e-link\") else None\n","        department = doctor_div.find('div', class_=\"nova-legacy-e-text nova-legacy-e-text--size-m nova-legacy-e-text--family-sans-serif nova-legacy-e-text--spacing-none nova-legacy-e-text--color-inherit\").get_text(strip=True)\n","        disciplines = doctor_div.find_all('li', class_=\"nova-legacy-e-list__item nova-legacy-v-person-list-item__info-section-list-item\")\n","        disciplines = extract_disciplines(disciplines)\n","        return doctor_id, doctor_name, department, disciplines\n","    except Exception as e:\n","        print(f\"Error extracting doctor info: {e}\")\n","        return None, None, None, None\n","\n","# Function to fetch doctor's profile page\n","def extract_doctor_profile_page(doctor_id):\n","    url = f\"https://www.researchgate.net/profile/{doctor_id}\"\n","    response = get_page_with_proxy(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    return soup\n","\n","# Function to extract statistics from profile page\n","def extract_stat(soup):\n","    try:\n","        publications = soup.find('div', attrs={'data-testid': 'publicProfileStatsPublications'})\n","        citations = soup.find('div', attrs={'data-testid': 'publicProfileStatsCitations'})\n","        reads = soup.find('div', attrs={'data-testid': 'publicProfileStatsReads'})\n","        return (\n","            publications.get_text(strip=True) if publications else \"0\",\n","            citations.get_text(strip=True) if citations else \"0\",\n","            reads.get_text(strip=True) if reads else \"0\",\n","        )\n","    except Exception as e:\n","        print(f\"Error extracting statistics: {e}\")\n","        return \"0\", \"0\", \"0\""],"metadata":{"id":"6qWXbZB79Jj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Steps functions"],"metadata":{"id":"uWBI5gGg-M0n"}},{"cell_type":"code","source":["def add_data_to_df(df, doctor_id, doctor_name, department, disciplines, publications, citations, reads):\n","    df.loc[len(df.index)] = [doctor_id, doctor_name, department, disciplines, publications, citations, reads]"],"metadata":{"id":"yoauFI2k-SMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_doctor_divs(df, doctor_divs, max_dcotors_per_page):\n","    if max_dcotors_per_page is not None:\n","        doctor_divs = doctor_divs[:max_dcotors_per_page]\n","    for doctor_div in doctor_divs:\n","        doctor_info = extract_doctor_basic_info(doctor_div)\n","        if doctor_info[0]:  # Add to DataFrame only if doctor_id exists\n","            doctor_id, doctor_name, department, disciplines = doctor_info\n","            profile_soup = extract_doctor_profile_page(doctor_id)\n","            publications, citations, reads = extract_stat(profile_soup)\n","            add_data_to_df(df, doctor_id, doctor_name, department, disciplines, publications, citations, reads)"],"metadata":{"id":"VZzn7Cr7-etZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_page(page_number, url_to_scrape, max_dcotors_per_page):\n","    print(f\"Scraping page {page_number}...\")\n","    response = get_page_with_proxy(f\"{url_to_scrape}/{page_number}\")\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    doctors_divs = extract_doctors_divs(soup)\n","\n","    # Create a DataFrame for this page\n","    df = pd.DataFrame(columns=[\"doctor_id\", \"doctor_name\", \"department\", \"disciplines\", \"publications\", \"citations\", \"reads\"])\n","    process_doctor_divs(df, doctors_divs, max_dcotors_per_page)\n","\n","    # Save the DataFrame to a CSV file\n","    df.to_csv(f\"research_gate_page_{page_number}.csv\", index=False)\n","    print(f\"Page {page_number} data saved to research_gate_page_{page_number}.csv\")"],"metadata":{"id":"BZLtJoDo-gdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multi-threading"],"metadata":{"id":"6xBOJebe-kY-"}},{"cell_type":"code","source":["def process_pages_concurrently(url_to_scrape, max_pages, max_dcotors_per_page):\n","    with ThreadPoolExecutor() as executor:\n","        futures = [executor.submit(process_page, page_number, url_to_scrape, max_dcotors_per_page) for page_number in range(1, max_pages + 1)]\n","        for future in futures:\n","            future.result()"],"metadata":{"id":"hrZBEfEG-m_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main function"],"metadata":{"id":"XM34rU-A-trd"}},{"cell_type":"code","source":["def scrapping_start(url_to_scrape, pages = None, doctors_by_page = None):\n","  # Main scraping logic\n","  try:\n","      response = get_page_with_proxy(url_to_scrape)\n","      soup = BeautifulSoup(response.content, 'html.parser')\n","      if pages is None:\n","          pages = get_maximuim_page_number(soup)\n","      maximuim_page_number = min(get_maximuim_page_number(soup), pages)\n","      # it will process each university page that ocontains members\n","      process_pages_concurrently(url_to_scrape, maximuim_page_number, doctors_by_page)\n","  except Exception as e:\n","      print(f\"Scraping failed: {e}\")"],"metadata":{"id":"D2H7jUpY-wz6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scrapping"],"metadata":{"id":"bn00FGvu_BU3"}},{"cell_type":"code","source":["url_to_scrape = dynamic_url_to_scrape(\"Lebanese_International_University\")\n","scrapping_start(url_to_scrape, 2 , 7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDrE1Nn3_EqX","executionInfo":{"status":"ok","timestamp":1737649742521,"user_tz":-120,"elapsed":59478,"user":{"displayName":"Ali Assi","userId":"09079658952282962985"}},"outputId":"d5e9049c-5347-4a9b-9454-689f64f90f33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping page 1...Scraping page 2...\n","\n","Page 1 data saved to research_gate_page_1.csv\n","Page 2 data saved to research_gate_page_2.csv\n"]}]}]}